<!doctype html>
 
<head>
    <meta charset="utf-8">
    <style>
        p.code {"Lucida Console", Monaco, monospace}
        /* usage: <p class="code"></p> */
    </style>
</head>

<body bgcolor="#E6E6FA">
<script>

easy = Array(
    "<b>P-Value</b> <br> <i>Statistics</i> <br><br> The probability of finding a more extreme example given that the null hypothesis is correct.",
    "<b>pi (up to 50)</b> <br> math <br> 3. 141 59 26 535 8979 323 846264 3383 279 502 884 1971 69399 37 510",
    "<b>Euler's Number</b> (up to 9) <br> <i>Math</i> <br><br> 2. 7 1828 1828"
        );


hard = Array(
    " <b>Communality</b> <br> <i>Multivariate Statistics</i> <br><br> That portion of the variance of the ith variable contributed by the m common factors is called the ith communality. <br> Var(X_i) = Communality + Specific Variance ",
    " <b>Uniqueness, Specific Variance</b> <br> <i>Multivariate Statistics</i> <br><br> That portion of Var(X_i) = sigma_ii due to the specific factor. <br> Var(X_i) = Communality + Specific Variance ",
    " <b>Beautiful Soup Example</b> <br> <i>python</i> <br><br> import requests <br>from bs4 import BeautifulSoup <br><br>userID = thisID <br>baseURL = 'https://www.youtube.com/user/'<br>userURL = baseURL + str(userID) <br>r = requests.get(userURL) <br>soup = BeautifulSoup(r.text) ",
    " <b>First Line in Script</b> <br> <i>python</i> <br><br> #!/usr/bin/env python ",
    "<b>Common Pandas Data Manipulations</b> <br> <i>python</i> <br><br> # add dummy variables <br> X_df = pd.get_dummies(X_df) <br><br> # read in data <br> df = pd.read_csv(myFile, header=None) <br><br> # drop column <br> df = df.drop('badCol', axis=1) <br><br> # rename columns <br> df.columns = names ",
    "<b>Correlation between Numeric Columns in Data Frame</b> <br> <i>R</i> <br><br> cor(df[sapply(df, is.numeric)])",
    "<b>aggregate() (SQL Group By in R)</b> <br> <i>R</i> <br><br> aggregate(df['count'], by=c(df['day'], df['month'],df['year']), FUN=sum)",
    " <b>Proper Python Function</b> <br> <i>python</i> <br><br> <DL> <DT> def myFunction(myParameter1,myParameter2=0): <DD>'''<br>This is an explanation of the function. <br> <br>:param myParameter1: explanation <br>:type myParameter1: type <br>:param myParameter2: explanation <br>:type myParameter2: type <br>:return: explanation <br>:rtype: return type <br>'''</DL>",
    "<b>Linear Regression in R</b> <br> <i>Statistics</i> <br><br> # Multiple Linear Regression Example <br> fit <- lm(y ~ x1 + x2 + x3, data=mydata) <br><br> # Other useful functions <br> summary(fit) # show results <br> coefficients(fit) # model coefficients <br> confint(fit, level=0.95) # CIs for model parameters <br> fitted(fit) # predicted values <br> residuals(fit) # residuals <br> anova(fit) # anova table <br> vcov(fit) # covariance matrix for model parameters <br> influence(fit) # regression diagnostics ",
    "<b>conditionals in python</b> <br> <i>python</i> <br><br>if a==b:<br>&nbsp;&nbsp;&nbsp;&nbsp;pass",
    "<b>Space in HTML</b> <br> <i> Computer Science </i> <br><br> & nbsp;<br>(without a space)",
    "<b>PCA in R</b> <br> <i>Statistics</i> <br><br>library(stats) <br><br> # Create Principal Componenets <br> track.c <- prcomp(track,scale=TRUE) <br><br># check standard deviations of the PCs <br>track.c$sdev <br><br># check PCs (matrix of variable loadings) <br>track.c$rotation <br><br># create a scree plot <br>plot(track.c$sdev**2) <br><br># check eigenvalues <br>track.c$sdev**2 <br><br># calculate value of first principal component across all countries <br>as.matrix(track)%*%track.c$rotation[,1]",
    "<b>Create Factors</b> <br> <i>R</i> <br><br> daylight$char.year <- factor(daylight$year)",
    "<b>Change Column Names</b> <br> <i> R </i> <br><br> new.names <- c('sunrise','sunset','month','day','year') <br> names(daylight) <- new.names",
    "<b>Merge Data Frames</b> <br> <i> R </i> <br><br> # add daylight data <br> df <- merge(x=df1,y=df2,by=c('year','month','day'),x.all=TRUE)",
    "<b>Add Boolean Column to Data Frame</b> <br> <i> R </i> <br><br> # Add is.working.hour <br> df$is.working.hour <- 0 <br> df[df$working.day==1 & df$hour>=9 & df$hour<17,]$is.working.hour <- 1",
    "<b>Drop Columns from Data Frame</b> <br> <i>R</i> <br><br> df <- df[-c(6,7,9,10)]",
    "<b>Create Lagged Features</b> <br> <i>R</i> <br> <br> # create lagged features (to include 1,2,3, and 24 hour lags) <br> df$lag.1  <- c(rep(NA, 1), head(df$count, -1)) <br> df$lag.2  <- c(rep(NA, 2), head(df$count, -2)) <br> df$lag.3  <- c(rep(NA, 3), head(df$count, -3)) <br> df$lag.24 <- c(rep(NA,24), head(df$count,-24))",
    "<b>Days of Week</b> <br> <i>R</i> <br><br> # add days of week <br> df$weekday <- as.factor(weekdays(as.Date(paste(df$year,df$month,df$day,sep='-')))) ",
    "<b>Create Date</b> <br> <i>R</i> <br><br> as.Date(paste(df$year,df$month,df$day,sep='-'))",
    "<b>Scatterplot Matrix</b> <br> <i>R</i> <br><br> pairs(~temp+atemp+count,data=df)",
    "<b>Count of Field</b> <br> <i>unix</i> <br><br> time tail +2 ~/Downloads/file.csv | cut -d, -f6  | sort | uniq -c | sort -rn",
    "<b>Start HTTP Server</b> <br> <i>Unix/Python</i> <br><br> python2.7 -m SimpleHTTPServer <br>(Available at localhost:8000)",
    "<b>Check for SSH keys on your computer</b> <br> <i>Unix</i> <br><br> # Lists the files in your .ssh directory, if they exist <br>ls -al ~/.ssh",
    "<b>Add Line to Plot</b> <br> <i>R</i> <br><br> # dotted, blue vertical line at y=0.3 <br>abline(v=0.3,lty=2,col='blue') <br><br># dot-dash, green horizontal line at x=0.3 <br>abline(h=0.3,lty=6,col='green')",
    "<b>Creating a Function</b> <br> <i>R</i> <br><br> myfunction <- function(arg1, arg2) {<br> &nbsp;&nbsp;&nbsp;&nbsp; # statements <br> &nbsp;&nbsp;&nbsp;&nbsp; return(object) <br>}",
    "<b>Unique Values from each Column in Data Frame</b> <br> <i>R</i> <br><br> apply(df, MARGIN=2, FUN=unique) <br># MARGIN=2 Specifies Column",
    "<b>Create CSV from Data Frame</b> <br> <i>R</i> <br><br> write.csv(<br>&nbsp;&nbsp;&nbsp;&nbsp; df, <br>&nbsp;&nbsp;&nbsp;&nbsp; file = \"seatbelts.csv\", <br>&nbsp;&nbsp;&nbsp;&nbsp; row.names = FALSE <br>)",
    "<b>Check Data Frame</b> <br> <i>R</i> <br><br> str(df) <br>summary(df)",
    "<b>Time Series Tests</b> <br> <i>R</i> <br><br> adf.test(count, alternative='stationary', k=24) <br>pp.test(count)",
    "<b>(Partial and not) AutoCorrelation Function Plots</b> <br> <i>R</i> <br><br> acf(t1,50,main=\"TS of count group by day\") <br>pacf(t1,50,main=\"TS of count group by day\")",
    "<b>Create Time Series Object</b> <br> <i>R</i> <br><br> count.ts <- ts(df_$count)",
    "<b>Transpose Matrix</b> <br> <i>R</i> <br><br> t(my.matrix)",
    "<b>Correlations</b> <br> <i>R</i> <br><br> cor(df[c('count','day.hours','temp')],use=\"complete.obs\")",
    "<b>Write to CSV File</b> <br> <i>R</i> <br><br> # write dfx to csv file <br>write.table(testx, file=\"testFinal.csv\", sep=\",\", row.names=FALSE)"
);


// change preferences
if (Math.random()<.98) {document.write(hard[Math.floor(Math.random()*hard.length)]) } else {document.write(easy[Math.floor(Math.random()*easy.length)]) }

testing = Array(
)
document.write("<br><br><br>",testing)

</script>

<!-- Notes

Create JSON from Data Frame
library(jsonlite)
json <- toJSON(
  df,
  dataframe = "rows",
  factor = "string",
  pretty = TRUE
)
cat(json, file = "seatbelts.json")


Random Forest
R
rf0 <- randomForest(
  count ~ 
    atemp      + day    + holiday + hour    + humidity
  + month      + season + temp    + weather + windspeed  
  + workingday + year,
  data=df.all,
  na.action=na.omit
)
plot(rf0)
plot(rf0$importance)
mean(rf0$mse) # Mean Squared Error = 381.28
mean(rf0$rsq) # Pseudo R Squared   = 0.988








Dummitize a Field
R
# season
dfx$s.spring <- 0
dfx[dfx$season=="1",]$s.spring <- 1
dfx$s.summer <- 0
dfx[dfx$season=="2",]$s.summer <- 1
dfx$s.fall <- 0
dfx[dfx$season=="3",]$s.fall <- 1
dfx$s.winter <- 0
dfx[dfx$season=="4",]$s.winter <- 1


Histogram
R
hist(df$count,breaks=20)


Dimensions
R
nrow()
ncol()


Investigating an object
R
mode(object)
class(object)


Concatenating Text
R
paste('{group:','"All"',', category:','1900',', measure:',nrow(movies[movies$year<1920 & movies$year>=1900,]),'},')


Invert a Matrix
R
inv(my.matrix)


Create a Matrix
R
matrix(c(1,2,3,2),nrow=2)


Definitions
    Logistic Regression
        f(x)
            the probability of the dependent variable equaling a "success" or "case" rather than a failure or non-case
        Odds
            the probability that a particular outcome is a case divided by the probability that it is a noncase
        Odds Ratio for a variable
            how the odds change with a one unit increase in that variable holding all other variables constant
    Probabilistic Graphical Model
        A graphical model or probabilistic graphical model (PGM) is a probabilistic model for which a graph expresses the conditional dependence structure between random variables.
    Cosine Similarity
        Cosine similarity is a measure of similarity between two vectors of an inner product space that measures the cosine of the angle between them.

    Three Axioms of Probability
    Learning Curve     
              |                         
        error |                         
              |                               
              |___________________
                training set size
        small difference between ==> high bias
    [For each Algorithm]
        [scalability]
        [fast training / evaluation]
        [flexibility, bias-variance tradeoff]
        [interpretability]
        [accuracy]
    lasso
        l1
        l2
        how do you tune the coefficient
            cross validation

    Lift definition:
        Ratio between the success rate and the base rate
        Equivalent to TPR divided by percent targeted
    power analysis for two independent proportions
    paired sample testing
    Bounce Rate (Google)
        click on website from Google search, then bounce back to search page
        go to your site and leave (bounce rate)
    HashTable
        Definition: data structure
            implements a structure that can map keys to values.
            supports efficient ``find queries''
        Uses a hash function to computes an array index from a key.
            collision resolution when two keys give the same index
        Efficient processing (O(1)) of find queries.
        Python: Dict and set are hash tables
    QuickSort
        divide and conquer
        1. partition array into subarrays and sort the sub arrays
        2. take first element and find the correct position
        3. recursively sort remaining arrays
    Model Considerations
        Bias v. Variance (Overfitting)
            Training MSE v. Test MSE
        Accuracy v. Interpretability
        Supervised v. Unsupervised
        Regression v. Classification
    
    Hypothesis Test
    Significance
    A/B Test
        Definition
            An A/B test involves testing two versions of a web page, an A version (the control) and a B version (the variation), with live traffic and measuring the effect each version has on your metric of interest (for example: conversion rate, click rate).
        In an A/B test, how can you check if assignment to the various buckets was truly random?
            One way to check is to look at metrics that should not be changed by the experiments and test that the distributions are similar in the two buckets (example: demographic data, number of visitors, browser version, time of assignment). The best way to test is to run A/A experiments. An A/A test is simply a test where both pages/ad copy/creatives/etc. are exactly the same. You split your traffic randomly and showing both groups the same page. A/A will help you check your split testing tool and help you understand the expected variation in your metrics.
        Process
            Define Success
                quantifiable core metrics
            identify bottleknecks
            construct a hypothesis
                if I change X, we may see an improvement in...
            prioritize
            test



Examples
    In
        Question
            def find(L,x)
                for e in L:
                    if e==x:
                        return True
                return False
            What is the complexity of 'in' in Python?
        Answer
            list - Average: O(n)
            set/dict - Average: O(1), Worst: O(n)
    You have 100 .csv files in a folder, and they all have the same header. The files are named data1, …, data100. We want all these .csv files stored in one data frame. How would you do that in R?
        1. store file names is a character vector
        2. use an apply statement to read the files into a data.frame (result: list of data frames)
        3. concatenate all these data frame into a single big one
        # something like this
        files.names = paste('data', 1:100, '.csv', sep='')
        tables = lapply(files.names, function(file){read.csv(file, header=T)}
        do.call(rbind, tables) ## rbindlist should have a better performance
    Suppose you are working at a casino, and the casino manager asks you if a single die is fair to use. How would you go about confirming that?
        Step 1: Throw the die a few of times, say N and record the results. (what N should be?)
            Say you get X = (X_1, … X_6) where X_i is the number of times you see i.
        Step 2: Do a formal hypothesis testing to check whether or not you should reject the null hypothesis of the die being fair. In this case we can do a Chi^2 test for multinomial data.
            Null Hypothesis: Die is fair.
            Under the null hypothesis X = (X_1, … X_6) is a multinomial (N, p=(⅙, …,⅙ )) distribution.
                Compute
                    T = \sum_k (X_i N/6)^2 / N/6
            Under H0 T is distributed Chi^2 with k1 degrees of freedom.
                p-value can be computed as P(Chi^2_k1 > T)
            If the p-value is below a threshold that I fixed in advance (say 0.01), I would reject that the coin is fair.
    Suppose you are working on a model and you get a high error:
        get more training examples ==> fixes high variance
        try smaller set of features ==> fixes high variance
        try getting additional features ==> fixes high bias
        try getting polynomial features ==> fixes high bias
        try increasing lambda (penalty term) ==> fixes high bias
        try decreasing lambda (penalty term) ==> fixes high variance
    Suppose you are working on a model and you get a high error:
        if you have high variance
            get more training examples
            try smaller set of features
            try decreasing lambda (penalty term)
        if you have high bias
            try getting additional features
            try getting polynomial features
            try increasing lambda (penalty term)
    What do you do with missing data?
        Leave it in (N/A)
            there might be information in keeping it
        remove rows
        replace with most common value / average
    What do you do when you have two classes and one is underrepresented?
    What would you do with a multi-class/label problem?
        one v. all
        one v. one


"""
import requests

def fetchData(startDate,endDate):
    '''
    Given the necessary parameters, fetch data by day

    :param startDate: first day
    :type startDate: string
    :param endDate: last day
    :type endDate: string
    :return: json object with info from api
    :rtype: json
    '''

    # base url for api call
    baseURL = 'http://api.com/'

    # dictionary of parameters for call
    params = {
        'group': 'day',
        'metric': 'views',
        'start': startDate,
        'end': endDate
    }

    # send request to API
    r = requests.get(baseURL,param=params)

    # return data in json
    return r.json()
"""

Latency numbers every programmer should know
    L1 cache reference ......................... 0.5 ns
    Branch mispredict ............................ 5 ns
    L2 cache reference ........................... 7 ns
    Mutex lock/unlock ........................... 25 ns
    Main memory reference ...................... 100 ns             
    Compress 1K bytes with Zippy ............. 3,000 ns  =   3 µs
    Send 2K bytes over 1 Gbps network ....... 20,000 ns  =  20 µs
    SSD random read ........................ 150,000 ns  = 150 µs
    Read 1 MB sequentially from memory ..... 250,000 ns  = 250 µs
    Round trip within same datacenter ...... 500,000 ns  = 0.5 ms
    Read 1 MB sequentially from SSD* ..... 1,000,000 ns  =   1 ms
    Disk seek ........................... 10,000,000 ns  =  10 ms
    Read 1 MB sequentially from disk .... 20,000,000 ns  =  20 ms
    Send packet CA->Netherlands->CA .... 150,000,000 ns  = 150 ms



def test(actual, expected):
    if (actual != expected):
        print "[FAILED] actual: {}, expected: {}".format(actual, expected)
 
if __name__ == "__main__":
    test(solution([]), -1)
    test(solution([2, -2, 3, 0, 4, -7]), 4)
 
    print "if you are seeing this, done"



Given two strings, write a method to decide if one is a permutation of the other
    Order the letters and compare (n*log(n))
    notes
        lower limit is n
How to check or detect duplicate elements in a list/array?
    go through array and make hash table counting
How to find the median of an array
    sort and find n/2
Find second lowest number
    go through once, saving lowest and second lowest
1s and 0s ordered
    binary search
        start in middle.  move left or right to middle as needed.
        log(n)
When is computation at least linear?
    When you need to look at every element
When do you get log(n)?
    When data is already sorted
In which cases you need at least n*log(n)
    When you need to sort
You have a stream of data coming in of size n, but you don't know what n is ahead of time. Write an algorithm that will take a random sample of k elements.  Can you write one that takes O(k) space?
    Start with k=1.  Solve.
    Start with k=2.  Solve.
    ...
    Generalize.














Random Forest in sklearn
Machine Learning

from sklearn.ensemble import RandomForestClassifier

# create random forest object
rf = RandomForestClassifier(
            n_estimators=32,
            max_depth=None,
            min_samples_split=2,
            min_samples_leaf=1,
            max_features=32,
            max_leaf_nodes=None,
            bootstrap=True,
            oob_score=True,
            n_jobs=1,
            random_state=None
      )

# train random forest
rf.fit(X_sample,y_sample)

# out of bag score
print 'out of bag score'
print rf.oob_score_

# Compute ROC curve and area the curve
fpr, tpr, thresholds = roc_curve(y_test, probas_[:, 1])
roc_auc = auc(fpr, tpr)
print "Area under the ROC curve : %f" % roc_auc

# save the classifier
with open('rf_clf2.pkl', 'wb') as fid:
    cPickle.dump(rf, fid)












Naive Bayes in sklearn
Machine Learning

from sklearn.naive_bayes import MultinomialNB

# create multinomial naive bayes object
nb = MultinomialNB(
            alpha=1,
            fit_prior=True,
            class_prior=None
        )

# top features, as given from testing with Random Forests
topFeatures = ['Education_ Bachelors degree(BA AB BS)', 'Education_ Children', 
                ...'NumUnder18_ Not in universe']

# get data
print 'get data'
X_train, y_train, X_test, y_test = cleaning.get_census()

# sample from training data using top Features
print 'sample from training data'
X_sample, y_sample = cleaning.create_sample_dataframe2(
                        X_train[topFeatures],
                        y_train,
                        nRows=120000,
                        proportionPositive=0.6
                     )

# measure naive bayes
cleaning.measure_cross_val(X=X_sample,y=y_sample,clf=nb,cv=3)

# Print Accuracy, Classification Report, and Confusion Matrix
print 'Accuracy, Classification Report, and Confusion Matrix'
cleaning.measure_performance(X_test[topFeatures], y_test, nb)












Measuring Performance in Supervised Learning in sklearn
Machine Learning

from sklearn import metrics

def measure_performance(X, y, clf, 
                        show_accuracy=True,
                        show_classification_report=True, 
                        show_confusion_matrix=True
    ):

    # predict targets based on X (to determine classifier's accuracy)
    y_pred = clf.predict(X)

    # print accuracy score
    if show_accuracy:
         print "Accuracy:{0:.3f}".format(metrics.accuracy_score(y, y_pred)),"\n"

    # print classification report
    if show_classification_report:
        print "Classification report"
        print metrics.classification_report(y, y_pred),"\n"

    # print confusion matrix
    if show_confusion_matrix:
        print "Confusion matrix"
        print metrics.confusion_matrix(y, y_pred),"\n"




Plot Two Functions
R
plot(fn1,col='black')
plot(fn2,col='red', add=TRUE)




Programming Defensively
Computer Science
(With respect to batch processing)
For errors, just log them and move on
Also count the number of errors
     if count exceeds some threshold, do something
        maybe exit job or generate error message


PySpark
python

PySpark is the Python API for Spark.

Public classes:
    SparkContext: Main entry point for Spark functionality.
    RDD: A Resilient Distributed Dataset (RDD), the basic abstraction in Spark.
    Broadcast: A broadcast variable that gets reused across tasks.
    Accumulator: An "add-only" shared variable that tasks can only add values to.
    SparkConf: For configuring Spark.
    SparkFiles: Access files shipped with jobs.
    StorageLevel: Finer-grained cache persistence levels.

